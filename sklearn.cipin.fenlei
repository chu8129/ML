#first extract the 20 news_group dataset to /scikit_learn_data
from sklearn.datasets import fetch_20newsgroups
#all categories
#newsgroup_train = fetch_20newsgroups(subset='train')
#part categories
categories = ['comp.graphics',
 'comp.os.ms-windows.misc',
 'comp.sys.ibm.pc.hardware',
 'comp.sys.mac.hardware',
 'comp.windows.x'];
newsgroup_train = fetch_20newsgroups(subset = 'train',categories = categories);

def calculate_result(actual,pred):
    m_precision = metrics.precision_score(actual,pred);
    m_recall = metrics.recall_score(actual,pred);
    print 'predict info:'
    print 'precision:{0:.3f}'.format(m_precision)
    print 'recall:{0:0.3f}'.format(m_recall);
    print 'f1-score:{0:.3f}'.format(metrics.f1_score(actual,pred));
    

#print category names
from pprint import pprint
pprint(list(newsgroup_train.target_names))



#newsgroup_train.data is the original documents, but we need to extract the 
#TF-IDF vectors inorder to model the text data
from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer
#vectorizer = TfidfVectorizer(sublinear_tf = True,
#                           max_df = 0.5,
#                           stop_words = 'english');
#however, Tf-Idf feather extractor makes the training set and testing set have
#divergent number of features. (Because they have different vocabulary in documents)
#So we use HashingVectorizer
vectorizer = HashingVectorizer(stop_words = 'english',non_negative = True,
                               n_features = 100)
fea_train = vectorizer.fit_transform(newsgroup_train.data)
#return feature vector 'fea_train' [n_samples,n_features]
print 'Size of fea_train:' + repr(fea_train.shape)
#11314 documents, 130107 vectors for all categories
print 'The average feature sparsity is {0:.3f}%'.format(
fea_train.nnz/float(fea_train.shape[0]*fea_train.shape[1])*100);


######################################################
#Multinomial Naive Bayes Classifier
print '*************************\nNaive Bayes\n*************************'
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
newsgroups_test = fetch_20newsgroups(subset = 'test',
                                     categories = categories);
fea_test = vectorizer.fit_transform(newsgroups_test.data);
#create the Multinomial Naive Bayesian Classifier
clf = MultinomialNB(alpha = 0.01) 
clf.fit(fea_train,newsgroup_train.target);
pred = clf.predict(fea_test);
calculate_result(newsgroups_test.target,pred);
#notice here we can see that f1_score is not equal to 2*precision*recall/(precision+recall)
#because the m_precision and m_recall we get is averaged, however, metrics.f1_score() calculates
#weithed average, i.e., takes into the number of each class into consideration.

######################################################
#KNN Classifier
from sklearn.neighbors import KNeighborsClassifier
print '*************************\nKNN\n*************************'
knnclf = KNeighborsClassifier()#default with k=5
knnclf.fit(fea_train,newsgroup_train.target)
pred = knnclf.predict(fea_test);
calculate_result(newsgroups_test.target,pred);

######################################################
#SVM Classifier
from sklearn.svm import SVC
print '*************************\nSVM\n*************************'
svclf = SVC(kernel = 'linear')#default with 'rbf'
svclf.fit(fea_train,newsgroup_train.target)
pred = svclf.predict(fea_test);
calculate_result(newsgroups_test.target,pred);

######################################################
#KMeans Cluster
from sklearn.cluster import KMeans
print '*************************\nKMeans\n*************************'
pred = KMeans(n_clusters=5)
pred.fit(fea_test)
calculate_result(newsgroups_test.target,pred.labels_);


####################################################### 
"""

http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz
#first extract the 20 news_group dataset to /scikit_learn_data  
from sklearn.datasets import fetch_20newsgroups  
#all categories  
#newsgroup_train = fetch_20newsgroups(subset='train')  
#part categories  
categories = ['comp.graphics',  
 'comp.os.ms-windows.misc',  
 'comp.sys.ibm.pc.hardware',  
 'comp.sys.mac.hardware',  
 'comp.windows.x'];  
newsgroup_train = fetch_20newsgroups(subset = 'train',categories = categories);  

#print category names
from pprint import pprint
pprint(list(newsgroup_train.target_names))
结果：
['comp.graphics',
 'comp.os.ms-windows.misc',
 'comp.sys.ibm.pc.hardware',
 'comp.sys.mac.hardware',
 'comp.windows.x']

load进来的newsgroup_train就是一篇篇document，我们要从中提取feature，即词频啊神马的，用fit_transform
Method 1. HashingVectorizer，规定feature个数

#newsgroup_train.data is the original documents, but we need to extract the   
#feature vectors inorder to model the text data  
from sklearn.feature_extraction.text import HashingVectorizer  
vectorizer = HashingVectorizer(stop_words = 'english',non_negative = True,  
                               n_features = 10000)  
fea_train = vectorizer.fit_transform(newsgroup_train.data)  
fea_test = vectorizer.fit_transform(newsgroups_test.data);  
  
  
#return feature vector 'fea_train' [n_samples,n_features]  
print 'Size of fea_train:' + repr(fea_train.shape)  
print 'Size of fea_train:' + repr(fea_test.shape)  
#11314 documents, 130107 vectors for all categories  
print 'The average feature sparsity is {0:.3f}%'.format(  
fea_train.nnz/float(fea_train.shape[0]*fea_train.shape[1])*100);  


Size of fea_train:(2936, 10000)
Size of fea_train:(1955, 10000)
The average feature sparsity is 1.002%
10000维feature，稀疏度还不算低
实际上用TfidfVectorizer统计可得到上万维的feature


Method 2. CountVectorizer+TfidfTransformer
CountVectorizer共享vocabulary：
#----------------------------------------------------  
#method 1:CountVectorizer+TfidfTransformer  
print '*************************\nCountVectorizer+TfidfTransformer\n*************************'  
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer  
count_v1= CountVectorizer(stop_words = 'english', max_df = 0.5);  
counts_train = count_v1.fit_transform(newsgroup_train.data);  
print "the shape of train is "+repr(counts_train.shape)  
  
count_v2 = CountVectorizer(vocabulary=count_v1.vocabulary_);  
counts_test = count_v2.fit_transform(newsgroups_test.data);  
print "the shape of test is "+repr(counts_test.shape)  
  
tfidftransformer = TfidfTransformer();  
  
tfidf_train = tfidftransformer.fit(counts_train).transform(counts_train);  
tfidf_test = tfidftransformer.fit(counts_test).transform(counts_test); 
结果
*************************
CountVectorizer+TfidfTransformer
*************************
the shape of train is (2936, 66433)
the shape of test is (1955, 66433)

Method 3. TfidfVectorizer
两个TfidfVectorizer共享vocabulary：
#method 2:TfidfVectorizer  
print '*************************\nTfidfVectorizer\n*************************'  
from sklearn.feature_extraction.text import TfidfVectorizer  
tv = TfidfVectorizer(sublinear_tf = True,  
                                    max_df = 0.5,  
                                    stop_words = 'english');  
tfidf_train_2 = tv.fit_transform(newsgroup_train.data);  
tv2 = TfidfVectorizer(vocabulary = tv.vocabulary_);  
tfidf_test_2 = tv2.fit_transform(newsgroups_test.data);  
print "the shape of train is "+repr(tfidf_train_2.shape)  
print "the shape of test is "+repr(tfidf_test_2.shape)  
analyze = tv.build_analyzer()  
tv.get_feature_names()#statistical features/terms  
结果：
*************************
TfidfVectorizer
*************************
the shape of train is (2936, 66433)
the shape of test is (1955, 66433)

3. 分类
3.1 Multinomial Naive Bayes Classifier
######################################################  
#Multinomial Naive Bayes Classifier  
print '*************************\nNaive Bayes\n*************************'  
from sklearn.naive_bayes import MultinomialNB  
from sklearn import metrics  
newsgroups_test = fetch_20newsgroups(subset = 'test',  
                                     categories = categories);  
fea_test = vectorizer.fit_transform(newsgroups_test.data);  
#create the Multinomial Naive Bayesian Classifier  
clf = MultinomialNB(alpha = 0.01)   
clf.fit(fea_train,newsgroup_train.target);  
pred = clf.predict(fea_test);  
calculate_result(newsgroups_test.target,pred);  
#notice here we can see that f1_score is not equal to 2*precision*recall/(precision+recall)  
#because the m_precision and m_recall we get is averaged, however, metrics.f1_score() calculates  
#weithed average, i.e., takes into the number of each class into consideration.  
f1≠2*（准确率*召回率）/（准确率+召回率）


函数calculate_result计算f1
def calculate_result(actual,pred):  
    m_precision = metrics.precision_score(actual,pred);  
    m_recall = metrics.recall_score(actual,pred);  
    print 'predict info:'  
    print 'precision:{0:.3f}'.format(m_precision)  
    print 'recall:{0:0.3f}'.format(m_recall);  
    print 'f1-score:{0:.3f}'.format(metrics.f1_score(actual,pred));  

3.2 KNN
######################################################  
#KNN Classifier  
from sklearn.neighbors import KNeighborsClassifier  
print '*************************\nKNN\n*************************'  
knnclf = KNeighborsClassifier()#default with k=5  
knnclf.fit(fea_train,newsgroup_train.target)  
pred = knnclf.predict(fea_test);  
calculate_result(newsgroups_test.target,pred);  

3.3 SVM
######################################################  
#SVM Classifier  
from sklearn.svm import SVC  
print '*************************\nSVM\n*************************'  
svclf = SVC(kernel = 'linear')#default with 'rbf'  
svclf.fit(fea_train,newsgroup_train.target)  
pred = svclf.predict(fea_test);  
calculate_result(newsgroups_test.target,pred);  



结果：
*************************
Naive Bayes
*************************
predict info:
precision:0.764
recall:0.759
f1-score:0.760
*************************
KNN
*************************
predict info:
precision:0.642
recall:0.635
f1-score:0.636
*************************
SVM
*************************
predict info:
precision:0.777
recall:0.774
f1-score:0.774



"""
